<!DOCTYPE html>
<html lang="en">

<head>
	<meta charset="utf-8">
	<meta name="viewport" width="device-width, initial-scale=1.0">
	<meta name="description" content="Publications">
	<meta name="author" content="Lei Shi">
	<title>Publications | Lei Shi </title>
	<!-- styles -->
	<style type="text/css"> li{margin-bottom: 0.5em;} </style>
	<link href="../css/bootstrap.css" rel="stylesheet">
	<link href="../css/jumbotron-narrow.css" rel="stylesheet">
	<link href="../css/style.css" rel="stylesheet">
</head>


<body>
<div class="container">

		<div class="header">
	        <ul class="nav nav-pills pull-right">
				<li class="passive"><a href="../research/research.html" title="Research">Research</a></li>
		        <li class="active"><a href="publications.html" title="Publications">Publications</a></li>
		        <li class="passive"><a href="../teaching/teaching.html" title="Teaching">Teaching</a></li>
                                         <li class="passive"><a href="../events/events.html" tilte="Grants">Grants</a> </li>
	        </ul>
	        <h3 class="text-muted" style="text-transform: none;"><a href="../index.html">Lei Shi (石磊) </a></h3>
        </div>

 <div class="cv">
			<h4>Published Papers </h4> <h5>(students underlined)</h5>

	                <ul>
			   <li>Truncated kernel stochastic gradient descent on spheres (with <u>J. Bai</u>),  <em>Mathematics of Computation </em> <strong>published online</strong>(2025), 72pp. 
			</li>
              <li>Single-step neural operator solver for semilinear evolution equations (with Z. Lei and X. Wang),  <em>Chinese Annals of Mathematics, Series B </em> <strong>46</strong>(2025), 321-340. 
			</li>
             <li>Nyström subsampling for functional linear regression (with J. Fan and <u>J. Liu</u>),  <em>Journal of Approximation Theory </em> <strong>310</strong>(2025), 106176 (25pp). 
			</li>

				
                        <li>A decentralized framework for kernel PCA with projection consensus constraints (with F. He, R. Yang and X. Huang),  <em>IEEE Transactions on Pattern Analysis and Machine Intelligence </em> <strong>47</strong>(2025), 3908-3921. 
			</li>
			
			<li>Distributed learning with discretely observed functional data (with <u>J. Liu</u>),  <em>Inverse Problems </em> <strong>41</strong>(2025), 045006 (45pp). 
			</li>


	                
			<li>Solving PDEs on spheres with physics-informed convolutional neural networks (with <u>G. Lei</u>, Z. Lei, C. Zeng and D.-X. Zhou),  <em>Applied and Computational Harmonic Analysis </em> <strong>74</strong>(2025), 101719 (40pp). 
			</li>
			
			
			<li>Pairwise ranking with Gaussian kernel (with <u>G. Lei</u>),  <em>Advances in Computational Mathematics </em> <strong>50</strong>(2024), 1-56. 
			</li>

			
			<li>Iterative kernel regression with preconditioning (with <u>Z. Zhang</u>),  <em>Analysis and Applications </em> <strong>22</strong>(2024), 1095-1131. 
			</li>

			<li>Spectral algorithms for functional linear regression (with J. Fan and Z.-C. Guo),  <em>Communications on Pure and Applied Analysis </em> <strong>23</strong>(2024), 895-915. 
			</li>

			<li>Optimality of robust online learning (with Z.-C. Guo and A. Christmann),  <em>Foundations of Computational Mathematics </em> <strong>24</strong>(2024), 1455-1483.
			</li>

			<li>Optimal gradient tracking for decentralized optimization (with <u>Z. Song</u>, S. Pu and M. Yan),  <em>Mathematical Programming </em> <strong>207</strong>(2024), 1-53.
			</li>

			<li>Statistical optimality of divide and conquer kernel-based functional linear regression (with <u>J. Liu</u>),  <em>Journal of Machine Learning Research </em> <strong>25</strong>(2024), 1-56. 
			</li>

                        <li>Classification with deep neural networks and logistic loss (with <u>Z. Zhang</u> and D.-X. Zhou),  <em>Journal of Machine Learning Research </em> <strong>25</strong>(2024), 1-117. 
			</li>
 
			<li>Provably accelerated decentralized gradient methods over unbalanced directed graphs (with <u>Z. Song</u>, S. Pu and M. Yan),  <em>SIAM Journal on Optimization </em> <strong>34</strong>(2024), 1131-1156. 
			</li>

			<li>Efficient kernel canonical correlation analysis using Nyström approximation (with Q. Fang, M. Xu and D.-X. Zhou),  <em>Inverse Problems </em> <strong>40</strong>(2024), 045007 (28pp). 
			</li>
			
                        <li>Coefficient-based regularized distribution regression (with Y. Mao and Z.-C. Guo),  <em>Journal of Approximation Theory </em> <strong>297</strong>(2024), 105995 (37pp). 
			</li>

			<li>Global search and analysis for the nonconvex two-level l1 Penalty (with F. He, M. He and X. Huang), <em>IEEE Transactions on Neural Networks and Learning Systems </em> <strong>35</strong>(2024), 3886-3899. 
			</li>

			<li>Capacity dependent analysis for functional online learning algorithms (with X. Guo and Z.-C. Guo),  <em>Applied and Computational Harmonic Analysis </em> <strong>67</strong>(2023), 101567 (30pp).
			</li>

			<li>Learning with asymmetric kernels: least squares and feature interpretation (with M. He, F. He, X. Huang and JAK. Suykens),  <em>IEEE Transactions on Pattern Analysis and Machine Intelligence </em> <strong>45</strong>(2023), 10044-10054.
			</li>
				
			<li>Distributed spectral pairwise ranking algorithms (with Z.-C. Guo and T. Hu),  <em>Inverse Problems </em> <strong>39</strong>(2023), 025003 (21pp).
			</li>
			
			<li>Communication-efficient topologies for decentralized learning with O(1) consensus rate (with <u>Z. Song</u> et al.),  <em>NeurIPS 2022.</em> 
			</li>
				
			<li>Solving parametric partial differential equations with deep rectified quadratic unit neural networks (with Z. Lei and C. Zeng),  <em>Journal of Scientific Computing </em> <strong>93</strong>(2022), 1-31.
			</li>
			
			<li>Compressed gradient tracking for decentralized optimization over general directed networks (with <u>Z. Song</u>, S. Pu and M. Yan),  <em>IEEE Transactions on Signal Processing</em> <strong>70</strong>(2022), 1775-1787.
			</li>

			<li>Regularized regression problem in hyper-RKHS for learning kernels (with F. Liu, X. Huang, J. Yang and JAK. Suykens),  <em>Journal of Machine Learning Research</em> <strong>22</strong>(2021), 1–38.
			</li>

			<li>Analysis of Regularized Least-Squares in Reproducing Kernel Krĕın Spaces (with F. Liu, X. Huang, J. Yang and JAK. Suykens), <em>Machine Learning </em> <strong>110</strong>(2021), 1145–1173.
			</li>

			<li>Fast algorithms for robust principal component analysis with an upper bound on the rank (with N. Sha and M. Yan), <em>Inverse Problems and Imaging </em> <strong>15</strong>(2021), 109-128.
			</li>

			<li>Realizing data features by deep nets (with Z.-C. Guo and S.-B. Lin), <em>IEEE Transactions on Neural Networks and Learning Systems </em> <strong>31</strong>(2020), 4036-4048.
			</li>

			<li>A double-variational Bayesian framework in random Fourier features for indefinite kernels (with F. Liu, X. Huang, J. Yang and JAK. Suykens), <em>IEEE Transactions on Neural Networks and Learning Systems </em> <strong>31</strong>(2020),2965-2979.
			</li>

			<li>Sparse SIR: optimal rates and adaptive estimation (with K. Tan and Z. Yu), <em>Annals of Statistics</em> <strong>48</strong>(2020), 64-85.
			</li>

		    <li>Sparse kernel regression with coefficient-based lq-regularization (with X. Huang, Y. Feng and JAK. Suykens), <em>Journal of Machine Learning Research</em> <strong>20</strong>(2019), 1-44.
			</li>

			<li>Fast and strong convergence of online learning algorithms (with Z.-C. Guo), <em>Advances in Computational Mathematics</em> <strong>45</strong>(2019), 2745-2770.
			</li>

			<li>Distributed learning with indefinite kernels, <em>Analysis and Applications</em> <strong>17</strong>(2019), 947-975.
			</li>

			<li>Optimal rates for coefficient-based regularized regression (with Z.-C. Guo), <em>Applied and Computational Harmonic Analysis</em> <strong>47</strong>(2019), 662-701.
			</li>

			<li>An RKHS approach to estimate individualized treatment rules based on functional predictors (with J. Fan and F. Lv), <em>Mathematical Foundations of Computing</em> <strong>2</strong>(2019), 169-181.
			</li>

			<li>Nyström subsampling method for coefficient-based regularized regression (with <u>L. Ma</u>  and Z. Wu), <em>Inverse Problems</em> <strong>35</strong>(2019), 075002 (40pp).
			</li>

			<li>Robust mixed one-bit compressive sensing (with X. Huang, Y. Huang, Y. Huang, F. He, A. Maier and M. Yan), <em>Signal Processing</em> <strong>162</strong>(2019), 161-168.
			</li>

			<li>Distributed learning with multi-penalty regularization (with Z.-C. Guo and S.-B. Lin), <em>Applied and Computational Harmonic Analysis</em> <strong>46</strong>(2019), 478-499.
			</li>

			<li>Gradient descent for robust kernel-based regression (with Z.-C. Guo and T. Hu), <em>Inverse Problems</em> <strong>34</strong>(2018), 065009 (29pp).
			</li>

			<li>Convergence of unregularized online learning algorithms (with Y. Lei and Z.-C. Guo), <em>Journal of Machine Learning Research</em> <strong>18</strong>(2018), 1-33.
			</li>

			<li>Pinball loss minimization for one-bit compressive sensing (with X. Huang, M. Yan and JAK. Suykens), <em>Neurocomputing</em> <strong>314</strong>(2018), 275-283.

			<li>Learning theory of distributed regression with bias corrected regularization kernel network (with Z.-C. Guo and Q. Wu), <em>Journal of Machine Learning Research</em> <strong>18</strong>(2017), 1-25.
			</li>

			<li>Solution path for pin-SVM classifiers with positive and negative tau value (with X. Huang and JAK Suykens), <em>IEEE Transactions on Neural Networks and Learning Systems</em> <strong>28</strong>(2017), 1584-1593.
			</li>

			<li>Learning rates for regularized least squares ranking algorithm (with Y. Zhao and J. Fan), <em>Analysis and Applications</em> <strong>15</strong>(2017), 815-836. 
			</li>

			<li>Nonconvex sorted l1 minimization for sparse approximation (with X. Huang and M. Yan), <em>Journal of the Operations Research Society of China</em> <strong>3</strong>(2015), 207-229.
			</li>

			<li>Learning with the maximum correntropy criterion induced losses for regression (with Y. Feng, X. Huang, Y. Yang and JAK. Suykens)，<em>Journal of Machine Learning Research</em> <strong>16</strong>(2015), 993-1034.
			</li>

			<li>Two-level l1 minimization for compressed sensing (with X. Huang, Y. Liu, S. Van Huffel and JAK. Suykens), <em>Signal Processing</em> <strong>108</strong>(2015), 205204, 459-475.
			</li>

			<li>Sequential minimal optimization for SVM with pinball loss (with X. Huang and JAK. Suykens), <em>Neurocomputing</em> <strong>149</strong>(2015), 1596-1603.
			</li>

			<li>Ramp loss linear programming support vector machine (with X. Huang and JAK. Suykens), <em>Journal of Machine Learning Research</em> <strong>15</strong>(2014), 2185-2211.
			</li>

			<li>Support vector machine classifier with pinball loss (with X. Huang and JAK. Suykens), <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> <strong>36</strong>(2014), 984-997.
			</li>

			<li>Quantile regression with l1-regularization and Gaussian kernels (with X. Huang, Z. Tian and JAK. Suykens)，<em>Advances in Computational Mathematics</em> <strong>40</strong>(2014), 517-551.
			</li>
				
			<li>Asymmetric ν-tube support vector regression (with X. Huang, K. Pelckmansand and JAK. Suykens), <em>Computational Statistics & Data Analysis</em> <strong>77</strong>(2014), 371-382.
			</li>
            
			<li>Asymmetric least squares support vector machine classifiers (with X. Huang and JAK. Suykens), <em>Computational Statistics & Data Analysis</em> <strong>70</strong>(2014), 395-405.
			</li>

			<li>Learning with coefficient-based regularization and l1-penalty (with Z.-C. Guo), <em>Advances in Computational Mathematics</em> <strong>39</strong>(2013), 493-510.
			</li>

			<li>Learning theory estimates for coefficient-based regularized regression, <em>Applied and Computational Harmonic Analysis</em> <strong>34</strong>(2013), 252-265.
			</li>

			<li>Non-uniform randomized sampling for multivariate approximation by high order Parzen windows (with X.-J. Zhou and D.-X. Zhou), <em>Canadian Mathematical Bulletin</em> <strong>54</strong>(2011), 566-576.
			</li>
			
			<li>Classification with non-iid sampling (with Z.-C. Guo), <em>Mathematical and Computer Modelling</em> <strong>54</strong>(2011), 1347-1364.
			</li>

			<li> Concentration estimates for learning with l1-regularizer and data dependent hypothesis spaces (with Y. Feng and D.-X. Zhou), <em>Applied and Computational Harmonic Analysis</em> <strong>31</strong>(2011), 286-302.
			</li>

			<li>Normal estimation on manifolds by gradient learning (with D.-X. Zhou), <em>Numerical Linear Algebra with Applications</em> <strong>18</strong>(2011), 249-259.
			</li>

			<li>Learning theory viewpoint of approximation by positive linear operators (with S. Lv), <em> Computers and Mathematics with Applications</em> <strong>60</strong>(2010), 3177-3186.
			</li>

			<li>Hermite learning with gradient data (with X. Guo and D.-X. Zhou), <em>Journal of Computational and Applied Mathematics</em> <strong>233</strong>(2010), 3046-3059.

			</li>
		   </ul>

		</div>

        <div class="footer">
	        <div class="row">
		        <div class="col-sm-8">
			    <p><small>&copy; Updated: June 2025</small></p>
		        </div>
	        </div>
        </div>
</div>
<!-- javascript at the end of the doc so page loads faster -->
<script src="https://code.jquery.com/jquery.js"></script>
<script src="../js/bootstrap.min.js"></script>
<script src="../js/library.js"></script>
<script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js"></script>

</body>

</html>

